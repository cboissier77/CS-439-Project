{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'another_curr' from '/Users/maissenbenjemaa/Documents/EPFL/optimization/Project/another_curr.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shapes as sh\n",
    "import another_curr as ac\n",
    "import importlib\n",
    "importlib.reload(sh)\n",
    "importlib.reload(ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "curri_level_1 , curri_level_2, curri_level_3 = sh.train_data(True , False)\n",
    "anti_curri_level_1 , anti_curri_level_2 , anti_curri_level_3 = sh.train_data(False , True)\n",
    "rand_level =  sh.train_data(False , False)\n",
    "standard_curr = ac.standard_curriculum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sh.test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# curriculum learning \n",
    "acc_curri_1_list = []\n",
    "times_curri_1_list = []\n",
    "curri_losses_1_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_1(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    curri_losses_1 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_1, criterion, optimizer)\n",
    "        curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_2, criterion, optimizer)\n",
    "        curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_3, criterion, optimizer)\n",
    "        curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_curri_1 = sh.evaluate(model, test_data)\n",
    "    time_curri_1 = end_time - start_time\n",
    "    acc_curri_1_list.append(acc_curri_1)\n",
    "    times_curri_1_list.append(time_curri_1)\n",
    "    curri_losses_1_list.append(curri_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/curri_losses_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(curri_losses_1_list, f)\n",
    "\n",
    "with open('pickle_lists/times_curri_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_curri_1_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_curri_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_curri_1_list, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# Anti_curriculum \n",
    "acc_anti_curri_1_list = []\n",
    "times_anti_curri_1_list = []\n",
    "anti_curri_losses_1_list =[]\n",
    "for i in range (10): \n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_1(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    anti_curri_losses_1 = []\n",
    "    start_time = time()\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_1, criterion, optimizer)\n",
    "        anti_curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_2, criterion, optimizer)\n",
    "        anti_curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_3, criterion, optimizer)\n",
    "        anti_curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_anti_curri_1 = sh.evaluate(model, test_data)\n",
    "    time_anti_curri_1 = end_time - start_time\n",
    "    acc_anti_curri_1_list.append(acc_anti_curri_1)\n",
    "    times_anti_curri_1_list.append(time_anti_curri_1)\n",
    "    anti_curri_losses_1_list.append(anti_curri_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/anti_curri_losses_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(anti_curri_losses_1_list, f)\n",
    "\n",
    "with open('pickle_lists/times_anti_curri_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_anti_curri_1_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_anti_curri_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_anti_curri_1_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# random \n",
    "acc_rand_1_list = []\n",
    "times_rand_1_list = []\n",
    "rand_losses_1_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    random_losses_1 = []\n",
    "    model = sh.ShapeCNN_1(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time()\n",
    "    for epoch in range(9):  \n",
    "        batch_losses, _ = sh.train(model, rand_level , criterion, optimizer)\n",
    "        random_losses_1.extend(batch_losses)\n",
    "    end_time = time()\n",
    "    acc_random_1 = sh.evaluate(model, test_data)\n",
    "    time_random_1 = end_time - start_time\n",
    "    acc_rand_1_list.append(acc_random_1)\n",
    "    times_rand_1_list.append(time_random_1)\n",
    "    rand_losses_1_list.append(random_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/rand_losses_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(rand_losses_1_list, f)\n",
    "\n",
    "with open('pickle_lists/times_rand_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_rand_1_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_rand_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_rand_1_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# Standard curriculum\n",
    "acc_std_curri_1_list = []\n",
    "times_std_curri_1_list = []\n",
    "std_curri_losses_1_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_1(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    std_curri_losses_1 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        batch_losses, _ = sh.train(model, standard_curr, criterion, optimizer)\n",
    "        std_curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_std_curri_1 = sh.evaluate(model, test_data)\n",
    "    time_std_curri_1 = end_time - start_time\n",
    "    acc_std_curri_1_list.append(acc_std_curri_1)\n",
    "    times_std_curri_1_list.append(time_std_curri_1)\n",
    "    std_curri_losses_1_list.append(std_curri_losses_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/std_curri_losses_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(std_curri_losses_1_list, f)\n",
    "\n",
    "with open('pickle_lists/times_std_curri_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_std_curri_1_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_std_curri_1_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_std_curri_1_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# curriculum learning \n",
    "acc_curri_2_list = []\n",
    "times_curri_2_list = []\n",
    "curri_losses_2_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_2(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    curri_losses_2 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_1, criterion, optimizer)\n",
    "        curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_2, criterion, optimizer)\n",
    "        curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_3, criterion, optimizer)\n",
    "        curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_curri_2 = sh.evaluate(model, test_data)\n",
    "    time_curri_2 = end_time - start_time\n",
    "    acc_curri_2_list.append(acc_curri_2)\n",
    "    times_curri_2_list.append(time_curri_2)\n",
    "    curri_losses_2_list.append(curri_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.807)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_curri_2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('curri_losses_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(curri_losses_2_list, f)\n",
    "\n",
    "with open('times_curri_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_curri_2_list, f)\n",
    "\n",
    "with open('acc_curri_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_curri_2_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# Anti_curriculum \n",
    "acc_anti_curri_2_list = []\n",
    "times_anti_curri_2_list = []\n",
    "anti_curri_losses_2_list =[]\n",
    "for i in range (10): \n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_2(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    anti_curri_losses_2 = []\n",
    "    start_time = time()\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_1, criterion, optimizer)\n",
    "        anti_curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_2, criterion, optimizer)\n",
    "        anti_curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_3, criterion, optimizer)\n",
    "        anti_curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_anti_curri_2 = sh.evaluate(model, test_data)\n",
    "    time_anti_curri_2 = end_time - start_time\n",
    "    acc_anti_curri_2_list.append(acc_anti_curri_2)\n",
    "    times_anti_curri_2_list.append(time_anti_curri_2)\n",
    "    anti_curri_losses_2_list.append(anti_curri_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.80195)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_anti_curri_2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('anti_curri_losses_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(anti_curri_losses_2_list, f)\n",
    "\n",
    "with open('times_anti_curri_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_anti_curri_2_list, f)\n",
    "\n",
    "with open('acc_anti_curri_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_anti_curri_2_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# random \n",
    "acc_rand_2_list = []\n",
    "times_rand_2_list = []\n",
    "rand_losses_2_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    random_losses_2 = []\n",
    "    model = sh.ShapeCNN_2(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time()\n",
    "    for epoch in range(9):  \n",
    "        batch_losses, _ = sh.train(model, rand_level , criterion, optimizer)\n",
    "        random_losses_2.extend(batch_losses)\n",
    "    end_time = time()\n",
    "    acc_random_2 = sh.evaluate(model, test_data)\n",
    "    time_random_2 = end_time - start_time\n",
    "    acc_rand_2_list.append(acc_random_2)\n",
    "    times_rand_2_list.append(time_random_2)\n",
    "    rand_losses_2_list.append(random_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.830275)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(acc_rand_2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('rand_losses_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(rand_losses_2_list, f)\n",
    "\n",
    "with open('times_rand_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_rand_2_list, f)\n",
    "\n",
    "with open('acc_rand_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_rand_2_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# Standard curriculum\n",
    "acc_std_curri_2_list = []\n",
    "times_std_curri_2_list = []\n",
    "std_curri_losses_2_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_2(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    std_curri_losses_2 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        batch_losses, _ = sh.train(model, standard_curr, criterion, optimizer)\n",
    "        std_curri_losses_2.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_std_curri_2 = sh.evaluate(model, test_data)\n",
    "    time_std_curri_2 = end_time - start_time\n",
    "    acc_std_curri_2_list.append(acc_std_curri_2)\n",
    "    times_std_curri_2_list.append(time_std_curri_2)\n",
    "    std_curri_losses_2_list.append(std_curri_losses_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/std_curri_losses_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(std_curri_losses_2_list, f)\n",
    "\n",
    "with open('pickle_lists/times_std_curri_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_std_curri_2_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_std_curri_2_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_std_curri_2_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# curriculum learning \n",
    "acc_curri_3_list = []\n",
    "times_curri_3_list = []\n",
    "curri_losses_3_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_3(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    curri_losses_3 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_1, criterion, optimizer)\n",
    "        curri_losses_3.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_2, criterion, optimizer)\n",
    "        curri_losses_3.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_3, criterion, optimizer)\n",
    "        curri_losses_3.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_curri_3 = sh.evaluate(model, test_data)\n",
    "    time_curri_3 = end_time - start_time\n",
    "    acc_curri_3_list.append(acc_curri_3)\n",
    "    times_curri_3_list.append(time_curri_3)\n",
    "    curri_losses_3_list.append(curri_losses_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/curri_losses_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(curri_losses_3_list, f)\n",
    "\n",
    "with open('pickle_lists/times_curri_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_curri_3_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_curri_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_curri_3_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# Anti_curriculum \n",
    "acc_anti_curri_3_list = []\n",
    "times_anti_curri_3_list = []\n",
    "anti_curri_losses_3_list =[]\n",
    "for i in range (10): \n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_3(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    anti_curri_losses_3 = []\n",
    "    start_time = time()\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_1, criterion, optimizer)\n",
    "        anti_curri_losses_3.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_2, criterion, optimizer)\n",
    "        anti_curri_losses_3.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, anti_curri_level_3, criterion, optimizer)\n",
    "        anti_curri_losses_3.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_anti_curri_3 = sh.evaluate(model, test_data)\n",
    "    time_anti_curri_3 = end_time - start_time\n",
    "    acc_anti_curri_3_list.append(acc_anti_curri_3)\n",
    "    times_anti_curri_3_list.append(time_anti_curri_3)\n",
    "    anti_curri_losses_3_list.append(anti_curri_losses_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/anti_curri_losses_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(anti_curri_losses_3_list, f)\n",
    "\n",
    "with open('pickle_lists/times_anti_curri_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_anti_curri_3_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_anti_curri_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_anti_curri_3_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# random \n",
    "acc_rand_3_list = []\n",
    "times_rand_3_list = []\n",
    "rand_losses_3_list =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    random_losses_3 = []\n",
    "    model = sh.ShapeCNN_3(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    start_time = time()\n",
    "    for epoch in range(9):  \n",
    "        batch_losses, _ = sh.train(model, rand_level , criterion, optimizer)\n",
    "        random_losses_3.extend(batch_losses)\n",
    "    end_time = time()\n",
    "    acc_random_3 = sh.evaluate(model, test_data)\n",
    "    time_random_3 = end_time - start_time\n",
    "    acc_rand_3_list.append(acc_random_3)\n",
    "    times_rand_3_list.append(time_random_3)\n",
    "    rand_losses_3_list.append(random_losses_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/rand_losses_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(rand_losses_3_list, f)\n",
    "\n",
    "with open('pickle_lists/times_rand_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(times_rand_3_list, f)\n",
    "\n",
    "with open('pickle_lists/acc_rand_3_list.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_rand_3_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 + SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# curriculum learning \n",
    "acc_curri_1_list_SGD = []\n",
    "times_curri_1_list_SGD = []\n",
    "curri_losses_1_list_SGD =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    model = sh.ShapeCNN_1(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    curri_losses_1 = []\n",
    "    start_time = time()\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_1, criterion, optimizer)\n",
    "        curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_2, criterion, optimizer)\n",
    "        curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        batch_losses, _ = sh.train(model, curri_level_3, criterion, optimizer)\n",
    "        curri_losses_1.extend(batch_losses)\n",
    "\n",
    "    end_time = time()\n",
    "    acc_curri_1 = sh.evaluate(model, test_data)\n",
    "    time_curri_1 = end_time - start_time\n",
    "    acc_curri_1_list_SGD.append(acc_curri_1)\n",
    "    times_curri_1_list_SGD.append(time_curri_1)\n",
    "    curri_losses_1_list_SGD.append(curri_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "with open('pickle_lists/curri_losses_1_list_SGD.pkl', 'wb') as f:\n",
    "    pickle.dump(curri_losses_1_list_SGD, f)\n",
    "\n",
    "with open('pickle_lists/times_curri_1_list_SGD.pkl', 'wb') as f:\n",
    "    pickle.dump(times_curri_1_list_SGD, f)\n",
    "\n",
    "with open('pickle_lists/acc_curri_1_list_SGD.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_curri_1_list_SGD, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - iteration\n",
      "2 - iteration\n",
      "3 - iteration\n",
      "4 - iteration\n",
      "5 - iteration\n",
      "6 - iteration\n",
      "7 - iteration\n",
      "8 - iteration\n",
      "9 - iteration\n",
      "10 - iteration\n"
     ]
    }
   ],
   "source": [
    "# random \n",
    "acc_rand_1_list_SGD = []\n",
    "times_rand_1_list_SGD = []\n",
    "rand_losses_1_list_SGD =[]\n",
    "for i in range (10):\n",
    "    print(f\"{i+1} - iteration\")\n",
    "    random_losses_1 = []\n",
    "    model = sh.ShapeCNN_1(num_classes=8)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "    start_time = time()\n",
    "    for epoch in range(9):  \n",
    "        batch_losses, _ = sh.train(model, rand_level , criterion, optimizer)\n",
    "        random_losses_1.extend(batch_losses)\n",
    "    end_time = time()\n",
    "    acc_random_1 = sh.evaluate(model, test_data)\n",
    "    time_random_1 = end_time - start_time\n",
    "    acc_rand_1_list_SGD.append(acc_random_1)\n",
    "    times_rand_1_list_SGD.append(time_random_1)\n",
    "    rand_losses_1_list_SGD.append(random_losses_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save the results\n",
    "\n",
    "with open('pickle_lists/rand_losses_1_list_SGD.pkl', 'wb') as f:\n",
    "    pickle.dump(rand_losses_1_list_SGD, f)\n",
    "\n",
    "with open('pickle_lists/times_rand_1_list_SGD.pkl', 'wb') as f:\n",
    "    pickle.dump(times_rand_1_list_SGD, f)\n",
    "\n",
    "with open('pickle_lists/acc_rand_1_list_SGD.pkl', 'wb') as f:\n",
    "    pickle.dump(acc_rand_1_list_SGD, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
